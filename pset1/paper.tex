\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fullpage}
\title{6.867 Problem Set 1}
\date{October 1, 2015}

\begin{document}
\maketitle
\section{Gradient Descent}
Gradient descent is a very popular numerical algorithm for finding the minimum value of a function. The algorithm works by starting witha guessed input value, and iterates towards the minimum. At each step, the algorithm computes the gradient at its input value, and shifts its guessed input along the direction opposite the gradient (the amount shifted is proportional of a parameter to the algorithm called the step size). When the difference between evaluations of the function at two consecutive input values falls below some convergence criterion, the algorithm stops, and the last input value is returned as the location of the minimum.

\subsection{Implementation and Testing}
We implemented gradient descent in 25 lines of Python, using the numpy library. 13 of these lines were an implementation of a function to numerically estimate the gradient of a function. After verifying the validity of the gradient function on a few analytically simple functions, we tested gradient descent on the following two functions:
\begin{align*}
f_1(x_1, x_2) &= (x_1-2)^2 + (x_2-2)^2\\
f_2(x_1, x_2) &= x_1^2 + x_2^2 + 4\sin (x_1 + x_2)
\end{align*}
$f_1$ has a unique minimum at $(x_1, x_2) = (2,2)$, whereas $f_2$ has multiple local minima: they are at $(-0.626, -0.626)$ and $(1.798, 1.798)$, the former of which is the global minimum. (There is a third point at which the gradient of $f_2$ is zero, but that point is a saddle point.)

When testing with $f_1$, we found that for values of the step size between 0.1 and 0.7 and starting guesses $-10^{9} \le x_1, x_2 \le 10^{9}$, the algorithm converged to the minimum of $(2,2)$ fairly quickly (under 200 iterations), even with convergence criteria as small as $10^{-8}$. For significantly larger starting guesses, our gradient algorithm failed due to rounding errors, and for much smaller step sizes, our algorithm needed more steps to converge. For large step sizes (greater than $0.8$), the algorithm would actually take longer, or if the step size is very large, fail to converge altogether. This is due to the descent ``overshooting the minimum" on each iteration of the algorithm (i.e. the iteration moves the input point in the direction of the minimum, but moves much too far).

Testing $f_2$ led to some similar results. We found that step sizes between 0.1 and 0.3 and starting values $-10^{9} \le x_1, x_2 \le 10^9$ allowed the algorithm to converge quickly for a convergence criterion of $10^{-8}$. Adjusting the step size outside of this range led to similar behavior in that the algorithm would not converge, but interestingly, for this function, the algorithm would often get stuck on the $x_1 = x_2$ line. This is perhaps due to the fact that $f_2$ is a symmetric function. 

However, when testing $f_2$, we noticed that sometimes the algorithm would terminate in one minimum, and sometimes it would terminate in the other. (We were unable to produce a nontrivial example of the algorithm terminating at the saddle point.) The general pattern for starting guesses near the origin ($-10 \le x_1, x_2 \le 10$) is that starting guesses will converge to the minimum nearest them. This pattern was not followed exactly when the starting guess was significantly farther away from the origin, but it was still roughly true. This behavior is unsurprising; one would expect most functions to be contoured such that the gradient at most locations ``points most closely" to the closest local minimum.
\end{document}
