\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Gradient Descent}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Implementation and Testing}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Comparison with more sophisticated algorithms}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Linear Basis Function Regression}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Minimizing SSE with gradient descent}{2}}
\newlabel{p2figure1}{{2.1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The maximum-likelihood polynomials ($M=3$) produced by gradient descent. The sin function used to generate the data is in green, the polynomial produced by the closed-form formula in red, and the polynomial produced by gradient descent in blue. Note that in the left figure, the red curve is almost invisible, as it is mostly covered by the blue curve. The figure on the left corresponds to a convergence criterion of $10^{-8}$, while the one on the right corresponds to a convergence criterion of $10^{-5}$. The \texttt  {fmin\_bfgs} function produced a figure similar to the one on the left.}}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Ridge Regression}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Implementation and Testing}{3}}
\newlabel{p2figure2}{{2.1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The maximum-likelihood function produced using trigonometric basis functions ($M=3$). This function (blue) does not actually appear to be more faithful to the original sin function (green) than the maximum-likelihood degree-3 polynomial (red).}}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Model Selection}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}BlogFeedback}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Least Absolute Deviation}{4}}
\newlabel{p3figure1}{{3.1}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Ridge regression with $M = 7, \lambda = 0.00128$}}{5}}
\newlabel{p4figure}{{4}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The fits with the lowest (left) and highest (right) cross-validation errors. The left fit used the values $M=2, \lambda = 2^{-3}$, while the right fit used the values $M=6, \lambda = 2^{-1}$. The fitted polynomial is shown in red, with the training set, cross-validation set, and test set shown in blue, red, and green, respectively.}}{6}}
